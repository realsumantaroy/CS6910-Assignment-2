{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8029718,
          "sourceType": "datasetVersion",
          "datasetId": 4732746
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "sc6910 RAW Code Part A",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "_jjjlikRvVnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_filters, size_filters, size_fc, activation='relu', filter_organization='double', batch_normalization='yes', dropout=0.0):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = F.relu\n",
        "        elif activation == 'gelu':\n",
        "            self.activation = F.gelu\n",
        "        elif activation == 'silu':\n",
        "            self.activation = F.silu\n",
        "        elif activation == 'mish':\n",
        "            self.activation = self.mish\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function. Choose from 'relu', 'gelu', 'silu', 'mish'.\")\n",
        "\n",
        "        if batch_normalization == 'yes':\n",
        "            self.use_batch_norm = True\n",
        "        else:\n",
        "            self.use_batch_norm = False\n",
        "\n",
        "        self.dropout_rate = dropout  # Set dropout rate\n",
        "\n",
        "        if filter_organization == 'same':\n",
        "            filter_sizes = [n_filters] * 5  # Same number of filters in all layers\n",
        "        elif filter_organization == 'double':\n",
        "            filter_sizes = [n_filters * (2**i) for i in range(5)]  # Double filters in each subsequent layer\n",
        "        elif filter_organization == 'halve':\n",
        "            filter_sizes = [n_filters // (2**i) for i in range(5)]  # Halve filters in each subsequent layer\n",
        "        else:\n",
        "            raise ValueError(\"Invalid filter organization. Choose from 'same', 'double', 'halve'.\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, filter_sizes[0], kernel_size=size_filters)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn1 = nn.BatchNorm2d(filter_sizes[0])\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(filter_sizes[0], filter_sizes[1], kernel_size=size_filters)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn2 = nn.BatchNorm2d(filter_sizes[1])\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(filter_sizes[1], filter_sizes[2], kernel_size=size_filters)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn3 = nn.BatchNorm2d(filter_sizes[2])\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv4 = nn.Conv2d(filter_sizes[2], filter_sizes[3], kernel_size=size_filters)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn4 = nn.BatchNorm2d(filter_sizes[3])\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Conv2d(filter_sizes[3], filter_sizes[4], kernel_size=size_filters)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn5 = nn.BatchNorm2d(filter_sizes[4])\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)  # Dropout layer with specified rate\n",
        "\n",
        "        self.fc_input_size = self._get_fc_input_size(size_filter_last=filter_sizes[4], kernel_conv=size_filters)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.fc_input_size, size_fc)\n",
        "        self.fc2 = nn.Linear(size_fc, 10)\n",
        "\n",
        "    def _get_fc_input_size(self, size_filter_last, stride_conv=1, stride_pool=2, kernel_conv=5, kernel_pool=2, input_size=244):\n",
        "        for _ in range(5):  # Number of conv-pool layers\n",
        "            input_size = ((input_size - kernel_conv) // stride_conv) + 1  # Adjusted for kernel size and stride of conv layers\n",
        "            input_size = ((input_size - kernel_pool) // stride_pool) + 1  # Adjusted for kernel size, stride, and pooling of pool layers\n",
        "        return size_filter_last * input_size * input_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.activation(self.bn1(self.conv1(x)))) if self.use_batch_norm else self.pool1(self.activation(self.conv1(x)))\n",
        "        x = self.pool2(self.activation(self.bn2(self.conv2(x)))) if self.use_batch_norm else self.pool2(self.activation(self.conv2(x)))\n",
        "        x = self.pool3(self.activation(self.bn3(self.conv3(x)))) if self.use_batch_norm else self.pool3(self.activation(self.conv3(x)))\n",
        "        x = self.pool4(self.activation(self.bn4(self.conv4(x)))) if self.use_batch_norm else self.pool4(self.activation(self.conv4(x)))\n",
        "        x = self.pool5(self.activation(self.bn5(self.conv5(x)))) if self.use_batch_norm else self.pool5(self.activation(self.conv5(x)))\n",
        "\n",
        "        x = self.dropout(x) if self.dropout_rate > 0 else x\n",
        "\n",
        "        x = x.view(-1, self.fc_input_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def mish(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))"
      ],
      "metadata": {
        "id": "nM6a_4Ruwo_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}